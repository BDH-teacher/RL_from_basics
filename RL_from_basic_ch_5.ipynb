{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/RL_from_basics/blob/main/RL_from_basic_ch_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Method"
      ],
      "metadata": {
        "id": "kjZz9FuKz0B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 import"
      ],
      "metadata": {
        "id": "YGSnrhsmy62w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pwHz0daovSu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid World 클래스"
      ],
      "metadata": {
        "id": "k_XiJPabmA6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f8eAOAhYovSy"
      },
      "outputs": [],
      "source": [
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1 # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_right(self):\n",
        "        self.y += 1\n",
        "        if self.y > 3:\n",
        "            self.y = 3\n",
        "\n",
        "    def move_left(self):\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "\n",
        "    def move_up(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "\n",
        "    def move_down(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            return True\n",
        "        else :\n",
        "            return False\n",
        "\n",
        "    def get_state(self):\n",
        "        return (self.x, self.y)\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 환경에 해당하는 GridWorld 클래스 정의\n",
        "   - Step 함수 : 에이전트로부터 액션을 받아서 상태 변이를 일으키고, 보상을 정해주는 함수\n",
        "   - is_done 함수 : 에피소드가 끝났는지 판별해주는 함수, 에이전트가 종료 상태에 도달했으면 True, 아니면 False를 리턴\n",
        "   - reset함수 : 에이전트가 종료 상태에 도달했을 때 처음상태로 돌려 놓는 함수"
      ],
      "metadata": {
        "id": "vJnyfqHrzDLZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JNvzTnuovSz"
      },
      "source": [
        "## Agent 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8P76rn1VovS0"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select_action(self):\n",
        "        coin = random.random()\n",
        "        if coin < 0.25:\n",
        "            action = 0\n",
        "        elif coin < 0.5:\n",
        "            action = 1\n",
        "        elif coin < 0.75:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 에이전트에 해당하는 클래스\n",
        "   - 4방향 uniform 랜덤 액션을 선택\n",
        "   - select_action 함수 : 각각 1/4확률로 4가지 액션 중 하나를 선택하는 함수"
      ],
      "metadata": {
        "id": "lmEbt4B1zwyx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO86V8lOovSx"
      },
      "source": [
        "## 메인 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rs_HwZGfovS1"
      },
      "outputs": [],
      "source": [
        "# 초기 셋팅\n",
        "\n",
        "env = GridWorld()\n",
        "agent = Agent()\n",
        "data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]] # 테이블 초기화\n",
        "gamma = 1.0\n",
        "reward = -1\n",
        "alpha = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgLYaiUNovS2",
        "outputId": "8cd3af3e-19e3-4abe-c75c-f933cd8a2704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:06<00:00, 7791.68it/s] \n"
          ]
        }
      ],
      "source": [
        "for k in trange(50000): # 총 5만 번의 에피소드 진행\n",
        "  done = False\n",
        "  history = []\n",
        "\n",
        "  # 에피소드 1회 진행\n",
        "  while not done:\n",
        "    action = agent.select_action()\n",
        "    (x,y), reward, done = env.step(action)\n",
        "    history.append((x,y,reward))\n",
        "\n",
        "  # env 초기화\n",
        "  env.reset()\n",
        "\n",
        "  # 에피소드 종료 후 테이블 업데이트\n",
        "  cum_reward = 0\n",
        "\n",
        "  for transition in history[::-1]: # history의 뒤쪽부터 차례차례 리턴을 계산\n",
        "    x, y, reward = transition\n",
        "    data[x][y] = data[x][y] + alpha * (cum_reward-data[x][y])\n",
        "    cum_reward = reward + gamma * cum_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVh70oM6ovS3",
        "outputId": "adbd53f0-32d6-45ea-ed93-dd0ad5593dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-62.57203295125681, -60.45246296319531, -53.73226841910136, -49.76126757346607]\n",
            "[-61.18306140378648, -58.9247882581445, -52.95649761610383, -47.69758443289825]\n",
            "[-60.628457532890565, -57.405001984758, -43.866333383549865, -31.38582596324584]\n",
            "[-59.42589290505005, -53.677216795074536, -33.39860927505324, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# 학습이 끝난 후 데이터를 출력해보기 위한 코드\n",
        "\n",
        "for row in data:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실제로 학습을 진행하는 메인 함수\n",
        "  - GrideWorld 클래스의 인스턴스인 env라는 변수 선언\n",
        "  - data 변수 선언 : 테이블에 각 상태의 가치를 임의의 값으로 초기화\n",
        "    - MC는 data를 업데이트하는 방식으로 진행됨\n",
        "  - gamma : 1, alpha : 0.001\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "- while loop는 랜덤 에이전트가 경험을 쌓는 과정이고, for loop는 쌓은 경험을 이용해 테이블을 업데이트함\n",
        "  - for loop안의 cum_reward는 리턴을 의미함\n",
        "\n",
        "<br/>\n",
        "\n",
        "- 리턴($G_t$)의 정의 <br/>\n",
        "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\cdots$ <br/>\n",
        "$= R_{t+1} + \\gamma \\left(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\cdots\\right)$ <br/>\n",
        "$= R_{t+1} + \\gamma G_{t+1}$\n",
        "\n",
        "   - 리턴 $G_{t+1}$과 $G_t$사이의 재귀적인 관계가 존재함\n",
        "   - cum_reward라는 변수에 $G_{t+1}$가 담겨 있기에 $\\gamma$가 곱해지고 $R_{t+1}$이 더해지며 $G_t$가 됨"
      ],
      "metadata": {
        "id": "X-6ylN030yW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Difference"
      ],
      "metadata": {
        "id": "Wxb7f9H_Y3KJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 import 부터 Agent 클래스 까지\n",
        "- 앞의 MC와 동일"
      ],
      "metadata": {
        "id": "D3OH_G1nY-I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메인 함수"
      ],
      "metadata": {
        "id": "anNxueQXY_0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    #TD\n",
        "    env = GridWorld()\n",
        "    agent = Agent()\n",
        "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "    gamma = 1.0\n",
        "    reward = -1\n",
        "    alpha = 0.01 # MC에 비해 큰 값을 사용"
      ],
      "metadata": {
        "id": "mwjqwduxbsWA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in trange(50000): # 총 5만번의 에피소드 진행\n",
        "  done = False\n",
        "  while not done:\n",
        "    x, y = env.get_state()\n",
        "    action = agent.select_action()\n",
        "    (x_prime, y_prime), reward, done = env.step(action)\n",
        "    x_prime, y_prime = env.get_state()\n",
        "\n",
        "    # 한 번의 step이 진행되자 마자 바로 테이블의 데이터를 업데이트 해줌\n",
        "    data[x][y] = data[x][y] + alpha*(reward+gamma*data[x_prime][y_prime]-data[x][y])\n",
        "  env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poNo28ckbxaS",
        "outputId": "bce3fdc0-2e84-44ac-aaea-ec7d23872bf3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:03<00:00, 13840.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in data:\n",
        "  print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGje6AJwcBCZ",
        "outputId": "25fd4ce4-62d8-40f8-99f4-f3e971b7807c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-59.23433158596225, -57.357720164197566, -53.97985606061329, -52.233553391839656]\n",
            "[-57.710650172122165, -54.36239719063453, -50.05296181821229, -45.93090100650767]\n",
            "[-54.88291811398165, -50.79613818411838, -41.902543046020654, -33.109063269480195]\n",
            "[-52.25350398897838, -46.28072535115985, -32.52640497786485, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MC에 비해 학습 코드가 간결함\n",
        "  - 한 번의 액션마다 데이터 테이블이 업데이트됨\n",
        "  - TD가 MC에 비해 학습의 변동성이 작은 덕분에 그만큼 큰 폭의 업데이트가 가능하기에 alpha의 값을 키울 수 있음"
      ],
      "metadata": {
        "id": "D_BTHc9jcqxg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('challenge')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b4db2a12af3ef8b0e30f6de14b6f9eeee638905350cc1d2be0bebab10d16d430"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kjZz9FuKz0B5",
        "YGSnrhsmy62w",
        "k_XiJPabmA6P",
        "2JNvzTnuovSz",
        "wO86V8lOovSx",
        "anNxueQXY_0A"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}