{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/RL_from_basics/blob/main/RL_from_basic_ch_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. MDP를 모를 때 최고의 정책 찾기"
      ],
      "metadata": {
        "id": "DHOtaI_Jpity"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pwHz0daovSu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- q(s,a)를 담기 위해 numpy라이브러리 import"
      ],
      "metadata": {
        "id": "M_pIyKTa3XtD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO86V8lOovSx"
      },
      "source": [
        "## 6.1 몬테카를로 컨트롤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CULCyJ7wovSy"
      },
      "source": [
        "### GridWorld 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f8eAOAhYovSy"
      },
      "outputs": [],
      "source": [
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "\n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 환경에 해당하는 클래스 정의 (GridWorld)\n",
        "   - step 함수 : 에이전트로 부터 액션을 받아서 다음 상태와 보상, 에피소드가 끝났는지 여부를 리턴해주는 함수\n",
        "   - 나머지 기타 함수들은 이 step 함수를 잘 동작하게 하기 위해 존재함\n",
        "      - 벽에 막혀 있을 때 벽의 방향으로 진행하는 액션 무시를 위한 move가 복잡해졌을 뿐 챕터 5와 거의 유사함"
      ],
      "metadata": {
        "id": "VTEDqpsE3uyJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JNvzTnuovSz"
      },
      "source": [
        "### Q_Agent 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8P76rn1VovS0"
      },
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # q벨류를 저장하는 변수. 모두 0으로 초기화.\n",
        "        self.eps = 0.9\n",
        "        self.alpha = 0.01\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0, 3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, history):\n",
        "        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트 한다\n",
        "        cum_reward = 0\n",
        "        for transition in history[::-1]:\n",
        "            s, a, r, s_prime = transition\n",
        "            x,y = s\n",
        "            # 몬테 카를로 방식을 이용하여 업데이트.\n",
        "            self.q_table[x,y,a] = self.q_table[x,y,a] + self.alpha * (cum_reward - self.q_table[x,y,a])\n",
        "            cum_reward = cum_reward + r\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여주는 함수\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 에이전트에 해당하는 객체를 정의함\n",
        "  - 에이전트 객체 내부에 q(s,a)의 값을 저장하기 위한 테이블 가지고 있으며, 실제로 에이전트가 액션을 선택할 때 사용됨\n",
        "  - select_action 함수 : 상태 s를 input으로 받아 s에서 알맞은 액션을 입실론 그리디 방식을 통해 선택함\n",
        "    - 이를 위한 epsilon이라는 값 존재\n",
        "  - update_table 함수 : epsilon의 값을 점차 조금씩 줄여주기 위해 필요한 함수\n",
        "  - show_table 함수 : 학습이 끝난 후에 상태별로 q(s,a)의 값이 가장 큰 액션을 뽑아서 보여주는 함수"
      ],
      "metadata": {
        "id": "qphgFGN64tXZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu5p9uEwovS1"
      },
      "source": [
        "### 초기 환경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rs_HwZGfovS1"
      },
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "agent = QAgent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_iJ_TuqovS2"
      },
      "source": [
        "### 에피소드 진행(학습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgLYaiUNovS2",
        "outputId": "8ed566ba-76ca-435f-db21-91d81501566c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 5096.11it/s]\n"
          ]
        }
      ],
      "source": [
        "for k in trange(1000): # 총 1,000 에피소드 동안 학습\n",
        "    done = False\n",
        "    history = []\n",
        "\n",
        "    # env 초기화\n",
        "    s = env.reset()\n",
        "\n",
        "    while not done: # 에피소드 1회 진행\n",
        "        a = agent.select_action(s)\n",
        "        s_prime, r, done = env.step(a)\n",
        "        history.append((s, a, r, s_prime))\n",
        "        s = s_prime\n",
        "\n",
        "    agent.update_table(history) # 히스토리를 이용하여 에이전트를 업데이트\n",
        "    agent.anneal_eps()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 하나의 에피소드가 끝날 때까지 history라는 변수에 상태 전이 과정을 모두 저장해 두었다가, 에피소드가 끝난 순간 해당 변수를 이용해 에이전트 내부의 q 테이블을 업데이트함\n",
        "- 그리고 epsilon의 값을 조금 씩 줄여줌"
      ],
      "metadata": {
        "id": "faQXtY7V579B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxm5kF_QovS3"
      },
      "source": [
        "### 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVh70oM6ovS3",
        "outputId": "c3386544-7cd3-479e-fbc5-ee67306cae07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 3. 2. 3. 1.]\n",
            " [3. 0. 0. 2. 2. 3. 3.]\n",
            " [2. 3. 0. 1. 0. 2. 3.]\n",
            " [2. 2. 3. 1. 0. 2. 3.]\n",
            " [2. 1. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ],
      "source": [
        "agent.show_table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyDirUvUovS6"
      },
      "source": [
        "## 6.2 TD 컨트롤 1 - SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vQVM5P-1kaD"
      },
      "source": [
        "### GridWorld 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "34a_VinJ1kaD"
      },
      "outputs": [],
      "source": [
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "\n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhfWxwZb1kaE"
      },
      "source": [
        "### Agent 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Pn5i-oCh1kaE"
      },
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x,y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        # SARSA 업데이트 식을 이용\n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + self.q_table[next_x,next_y,a_prime] - self.q_table[x,y,a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- update_table 함수 : MC에서는 에이전트가 경험한 history 전체를 인자로 받았지만, TD에서는 트랜지션(transition)을 인풋으로 받음\n",
        "\n",
        "- 트랜지션(transition) : 상태 전이 1번을 뜻함\n",
        "  - 상태 s에서 a를 해서 보상 r을 받고 상태 s'에 도달했다면 (s,a,r,s')이 하나의 트랜지션임"
      ],
      "metadata": {
        "id": "-paj2E9T7aa4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHs4iJ5J1kaE"
      },
      "source": [
        "### 초기 환경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VS6hN6MO1kaF"
      },
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "agent = QAgent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F0UV0ui1kaF"
      },
      "source": [
        "### 에피소드 진행 (학습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab81092e-9d26-4d01-cb68-7069ff420c44",
        "id": "c4xv1UWZ1kaF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 4497.75it/s]\n"
          ]
        }
      ],
      "source": [
        "for k in trange(1000):\n",
        "    done = False\n",
        "\n",
        "    s = env.reset()\n",
        "    while not done:\n",
        "        a = agent.select_action(s)\n",
        "        s_prime, r, done = env.step(a)\n",
        "        agent.update_table((s,a,r,s_prime))\n",
        "        s = s_prime\n",
        "    agent.anneal_eps()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- QAgent에서 update_table 함수를 호출하는 주기가 다름\n",
        "  - MC에서는 한 에피소드가 끝나고 update_table을 호출했지만, TD는 한 step마다 호출\n",
        "  - 트랜지션 데이터를 통해 Q_table 업데이트"
      ],
      "metadata": {
        "id": "wAUnTAjGRevA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4JVKdKp1kaF"
      },
      "source": [
        "### 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aed95fc-5504-46af-d22c-b8025083667c",
        "id": "br0YSHUx1kaF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 1. 3. 3. 2.]\n",
            " [3. 3. 0. 2. 2. 3. 3.]\n",
            " [2. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 3. 3.]\n",
            " [0. 1. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ],
      "source": [
        "agent.show_table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj5WK95hAu2n"
      },
      "source": [
        "## 6.3 TD 컨트롤 2 - Q러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C829uuaAu2o"
      },
      "source": [
        "### GridWorld 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m5G3NXzDAu2o"
      },
      "outputs": [],
      "source": [
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1 # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "\n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wDhlHwVAu2o"
      },
      "source": [
        "### Agent 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9TU_0QxYAu2o"
      },
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택해준다\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x,y = s\n",
        "        next_x, next_y = s_prime\n",
        "\n",
        "        # Q러닝 업데이트 식을 이용\n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함\n",
        "        self.eps = max(self.eps, 0.2)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q러닝은 기존에 SARSA와 비슷함\n",
        "  - update_table 함수와 anneal_eps 함수의 내부만 약간 수정"
      ],
      "metadata": {
        "id": "ns1G5cbDaf1f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o063_OQJAu2p"
      },
      "source": [
        "### 초기 환경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xDp9LHyCAu2p"
      },
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "agent = QAgent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CSD5t3UAu2p"
      },
      "source": [
        "### 에피소드 진행(학습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ff0433-b0e4-4585-85ee-14cc339b634c",
        "id": "pibUcDnJAu2p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 2410.17it/s]\n"
          ]
        }
      ],
      "source": [
        "for n_epi in trange(1000):\n",
        "    done = False\n",
        "\n",
        "    s = env.reset()\n",
        "    while not done:\n",
        "        a = agent.select_action(s)\n",
        "        s_prime, r, done = env.step(a)\n",
        "        agent.update_table((s,a,r,s_prime))\n",
        "        s = s_prime\n",
        "    agent.anneal_eps()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTC7-VIUAu2p"
      },
      "source": [
        "### 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198fdd89-0d48-42a7-e1f4-142be67889c5",
        "id": "NYPq0s7nAu2p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 2. 3. 2. 3.]\n",
            " [3. 3. 0. 2. 2. 2. 3.]\n",
            " [2. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 3.]\n",
            " [3. 2. 1. 1. 0. 2. 0.]]\n"
          ]
        }
      ],
      "source": [
        "agent.show_table()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ZaMRh9y2EtO"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('challenge')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b4db2a12af3ef8b0e30f6de14b6f9eeee638905350cc1d2be0bebab10d16d430"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wO86V8lOovSx",
        "CULCyJ7wovSy",
        "2JNvzTnuovSz",
        "fu5p9uEwovS1",
        "P_iJ_TuqovS2",
        "6vQVM5P-1kaD",
        "vhfWxwZb1kaE",
        "zHs4iJ5J1kaE",
        "-F0UV0ui1kaF",
        "G4JVKdKp1kaF",
        "Oj5WK95hAu2n"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}